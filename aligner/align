#!/usr/bin/env python
import optparse
import sys
from collections import defaultdict

#Initializer
def initialize():
	for (n, (f, e)) in enumerate(bitext):
		if n%1000==0:
			print "loading...",n

		for e_j in set(e):
			total_e[e_j] = 0.0

		for f_i in set(f):
			total_f[f_i] = 0.0

			for e_j in set(e):
				count_ef[(e_j,f_i)] = 0.0
				count_fe[(e_j,f_i)] = 0.0
				t_ef[(e_j,f_i)]=(1.00/len(e))
		t_ef[e[0],f[0]] = 10
	print 'done loading'

def bayesian(iterations):
	for b in range(iterations):
		for (n,(f,e)) in enumerate(bitext):
			for (i, f_i) in enumerate(f): 
				for (j, e_j) in enumerate(e):

#EM Trainer in Reverse
def trainReverse(iterations):
		
	for b in range(iterations):

		#set all totals to 0
		for e in set(total_e):
			total_e[e]=0.0

		#set all counts to 0
		for (e,f) in set(count_fe):
			count_fe[(e,f)]=0.0
			
		#for all sentence pairs
		for (f,e) in bitext:
			
			for f_i in set(f):
				se_total=0.0
				
				for e_j in set(e):
					se_total += t_ef[(e_j,f_i)]
				
				for e_j in set(e):
					count_fe[(f_i,e_j)] += (t_ef[(e_j,f_i)] / se_total)
					total_e[e_j] += (t_ef[(e_j,f_i)] / se_total)

		#estimate probablities
		for e_j,f_i in t_ef:
			t_ef[(e_j,f_i)] = (count_fe[(f_i,e_j)]/total_e[e_j])
				
		print "iteration:", b

#EM Trainer
def train(iterations):
		
	for b in range(iterations):

		#set all totals to 0
		for f in set(total_f):
			total_f[f]=0.0

		#set all counts to 0
		for (e,f) in set(count_ef):
			count_ef[(e,f)]=0.0
			
		#for all sentence pairs
		for (f,e) in bitext:
			
			for e_j in set(e):
				se_total=0.0
				
				for f_i in set(f):
					se_total += t_ef[(e_j,f_i)]
				
				for f_i in set(f):
					count_ef[(e_j,f_i)] += (t_ef[(e_j,f_i)] / se_total)
					total_f[f_i] += (t_ef[(e_j,f_i)] / se_total)

		#estimate probablities
		for e_j,f_i in t_ef:
			t_ef[(e_j,f_i)] = (count_ef[(e_j,f_i)]/total_f[f_i])
				
		print "iteration:", b

	
def writeOutput(x):
	outfile = open('final.a','w') 
	print "printing to file..."
	for (f, e) in bitext:
	  for (i, f_i) in enumerate(f): 
	  	  best_p=0
	  	  best_j=0
		  for (j, e_j) in enumerate(e):
		      if t_ef[(e_j,f_i)] > best_p:
		      	      best_p=t_ef[(e_j,f_i)]
		      	      best_j=j
	          outfile.write("%i-%i " % (i,best_j))
	  outfile.write("\n")
	outfile.close()
	return


optparser = optparse.OptionParser()

optparser.add_option("-d", "--data", dest="train", default="data/hansards", help="Data filename prefix (default=data)")
optparser.add_option("-e", "--english", dest="english", default="e", help="Suffix of English filename (default=e)")
optparser.add_option("-o", "--out", dest="out", default="m1.", help="output prefid (default=m1.)")
optparser.add_option("-f", "--french", dest="french", default="f", help="Suffix of French filename (default=f)")
optparser.add_option("-t", "--threshold", dest="threshold", default=0.5, type="float", help="Threshold for aligning with Dice's coefficient (default=0.5)")
optparser.add_option("-n", "--num_sentences", dest="num_sents", default=sys.maxint, type="int", help="Number of sentences to use for training and alignment")
(opts, _) = optparser.parse_args()

f_data = "%s.%s" % (opts.train, opts.french)
e_data = "%s.%s" % (opts.train, opts.english)
bitext = [[sentence.strip().split() for sentence in pair] for pair in zip(open(f_data), open(e_data))[:opts.num_sents]]

#
total_f = defaultdict(float)
count_ef = defaultdict(float)
t_ef = defaultdict(float)

#reversing
total_e = defaultdict(float)
count_fe = defaultdict(float)

dict_eng=defaultdict(int)
dict_fr=defaultdict(int)

#initialize
initialize()

#EM
train(5)

#EM Reverse
trainReverse(5)

#Baysian
bayesian(5)

#Write out
writeOutput('final')