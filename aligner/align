#!/usr/bin/env python
import optparse
import sys
from collections import defaultdict

#Initializer
def initialize():
	print 'loading data'
	for (f, e) in (bitext):
		len_f = len(f)				
		len_e = len(e)
		for (i,f_i) in enumerate(f):
			total_f[f_i] = 0.0

			for (j,e_j) in enumerate(e):

				total_e[e_j] = 0.0
				count_ef[(e_j,f_i)] = 0.0
				count_fe[(e_j,f_i)] = 0.0
				t_ef[(e_j,f_i)]=1.0
				a[i,j,len_e,len_f] = (1.0 / (len_e+1.0))
		t_ef[e[0],f[0]] = 10
	print 'done loading'

#EM Trainer in Reverse
def trainReverse(iterations):
	print 'start backward model 1 training'
	for b in range(iterations):

		#set all totals to 0
		for e in set(total_e):
			total_e[e]=0.0

		#set all counts to 0
		for (e,f) in set(count_fe):
			count_fe[(e,f)]=0.0
			
		#for all sentence pairs
		for (f,e) in bitext:
			
			for f_i in set(f):
				se_total=0.0
				
				for e_j in set(e):
					se_total += t_ef[(e_j,f_i)]
				
				for e_j in set(e):
					count_fe[(f_i,e_j)] += (t_ef[(e_j,f_i)] / se_total)
					total_e[e_j] += (t_ef[(e_j,f_i)] / se_total)

		#estimate probablities
		for e_j,f_i in t_ef:
			t_ef[(e_j,f_i)] = (count_fe[(f_i,e_j)]/total_e[e_j])
				
		print "iteration:", b
	print 'end backward model 1 training'
#EM Trainer
def train(iterations):
	print 'start forward model 1 training'
	for b in range(iterations):

		#set all totals to 0
		for f in set(total_f):
			total_f[f]=0.0

		#set all counts to 0
		for (e,f) in set(count_ef):
			count_ef[(e,f)]=0.0
			
		#for all sentence pairs
		for (f,e) in bitext:
			
			for e_j in set(e):
				se_total=0.0
				
				for f_i in set(f):
					se_total += t_ef[(e_j,f_i)]
				
				for f_i in set(f):
					count_ef[(e_j,f_i)] += (t_ef[(e_j,f_i)] / se_total)
					total_f[f_i] += (t_ef[(e_j,f_i)] / se_total)

		#estimate probablities
		for e_j,f_i in t_ef:
			t_ef[(e_j,f_i)] = (count_ef[(e_j,f_i)]/total_f[f_i])
				
		print "iteration:", b
	print 'end model 1 training'
#Model 2 Trainer
def train2(iterations):
	print 'start model 2 training'
	for b in range(iterations):

		#for all sentence pairs
		for (f,e) in bitext:
			len_e = len(e)
			len_f = len(f)
			for (j,e_j) in enumerate(e):
				total_sf[(e_j)]=0.0
				
				for (i,f_i) in enumerate(f):
					total_sf[(e_j)] += t_ef[(e_j,f_i)] * a[(i,j,len_e,len_f)]
				
				
				for (i,f_i) in enumerate(f):
					c = t_ef[(e_j,f_i)] * a[(i,j,len_e,len_f)] / total_sf[(e_j)]
					count[(e_j,f_i)] += c
					counts_d[(i,j,len_f,len_e)] += c
					total[(f_i)] += c
					total_d[(j,len_f,len_e)] +=c

		#smoothing
		laplace = 1.0
		for val in counts_d:
			if counts_d[val] > 0 and counts_d[val] < laplace:
				laplace = counts_d[val]

		laplace = laplace/2.0
		for val in counts_d:
			counts_d[val] += laplace

		for (j,f,e) in (total_d):
			total_d[(j,f,e)] += laplace * e

		#estimate probablities
		for (e_j,f_i) in count:
			t_ef[(e_j,f_i)] = count[(e_j,f_i)] / total[(f_i)]
		for (i,j,f,e) in counts_d:
			a[(i,j,e,f)] += counts_d[(i,j,f,e)] / total_d[(j,f,e)]

		print "iteration:", b
	print 'end model 2 training'
def writeOutput(x):
	outfile = open('final.a','w') 
	print "printing to file..."
	for (f, e) in bitext:
	  for (i, f_i) in enumerate(f): 
	  	  best_prob=0
	  	  best_alignment=0
		  for (j, e_j) in enumerate(e):
		      if t_ef[(e_j,f_i)] > best_prob:
		      	      best_prob=t_ef[(e_j,f_i)]
		      	      best_alignment=j
	          outfile.write("%i-%i " % (i,best_alignment))
	  outfile.write("\n")
	outfile.close()
	return


optparser = optparse.OptionParser()

optparser.add_option("-d", "--data", dest="train", default="data/hansards", help="Data filename prefix (default=data)")
optparser.add_option("-e", "--english", dest="english", default="e", help="Suffix of English filename (default=e)")
optparser.add_option("-o", "--out", dest="out", default="m1.", help="output prefid (default=m1.)")
optparser.add_option("-f", "--french", dest="french", default="f", help="Suffix of French filename (default=f)")
optparser.add_option("-t", "--threshold", dest="threshold", default=0.5, type="float", help="Threshold for aligning with Dice's coefficient (default=0.5)")
optparser.add_option("-n", "--num_sentences", dest="num_sents", default=sys.maxint, type="int", help="Number of sentences to use for training and alignment")
(opts, _) = optparser.parse_args()

f_data = "%s.%s" % (opts.train, opts.french)
e_data = "%s.%s" % (opts.train, opts.english)
bitext = [[sentence.strip().split() for sentence in pair] for pair in zip(open(f_data), open(e_data))[:opts.num_sents]]

#Model 1 Variables
total_f = defaultdict(float)
count_ef = defaultdict(float)
t_ef = defaultdict(float)

#Model 1 Reverse Variables
total_e = defaultdict(float)
count_fe = defaultdict(float)

dict_eng=defaultdict(int)
dict_fr=defaultdict(int)

# Model 2 Variables
a = defaultdict(float)
counts_d = defaultdict(float)
total_d = defaultdict(float)
total = defaultdict(float)
count = defaultdict(float)
total_sf = defaultdict(float)

#initialize
initialize()

#EM
train(5)

#EM Reverse
trainReverse(5)

train2(5)

#Write out
writeOutput('final')